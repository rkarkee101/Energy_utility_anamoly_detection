# Energy_utility_anomaly_detection

Monitoring school energy usage is crucial for identifying inefficiencies, equipment malfunctions, or underperforming retrofits. Anomalies in energy data can appear as single outlier points (e.g. a sudden spike in usage) or as subtle trends over time (e.g. a sustained period of slightly higher consumption). In anomaly detection literature, these are often called point anomalies and collective anomalies respectively. A collective anomaly refers to a set of data points that deviate from expectations only when considered together (even if individual points seem normal).
In the context of school utility bills (monthly energy usage), point anomalies might indicate events like equipment left running, while small trend anomalies could signal one building’s performance gradually worsening. The following sections provide two Python code examples to detect such anomalies using data-driven techniques:

•	Part 1: Detect anomalies (both point outliers and small trend deviations) in a single school’s energy usage time series (general utility bill data).
•	Part 2: Analyze multiple schools’ energy data before and after efficiency retrofits to identify any underperforming buildings (those that did not improve as expected), again capturing both point and trend anomalies in their performance.

Throughout, we normalize energy use by building size (energy per square foot, also known as Energy Use Intensity, EUI) for fair comparisons. We leverage advanced machine learning-based anomaly detection methods (e.g. Isolation Forest, one-class SVM) which have been shown to work well for energy data, and we discuss how to catch subtle collective anomalies (using sequences or trend models).



## Part 1: Detecting Anomalies in a Single Building’s Utility Time Series
For a single school’s monthly utility data, we want to flag: - Point anomalies: individual months with abnormally high or low usage. - Small trend anomalies (collective anomalies): a group of months showing a slight but sustained deviation (e.g. a 3-month upward trend in energy use that is unusual).
Approach:
1. Load and Prepare Data: Read the CSV file containing monthly utility data (electricity kWh, gas, dates, etc.). If the file contains multiple schools, filter for the specific school of interest. Ensure the data is in chronological order and handle missing values if any.
2. Feature Engineering: For time-series anomaly detection, capturing seasonality is important. You may create features like month-of-year or use year-over-year comparisons. In this example, we’ll focus on electricity usage (kWh) and incorporate the month as a feature for seasonal context.
3. Apply Anomaly Detection Model: Use an unsupervised ML model to detect outliers. We choose Isolation Forest (an ensemble method that isolates anomalies in the data) to detect point anomalies. Isolation Forest is effective for detecting outliers in energy consumption data. We set a contamination rate (expected proportion of anomalies) to guide the model.
4. Identify Anomalies: The model will assign an anomaly score to each point. We flag points with the lowest scores (predicted as outliers) as point anomalies.
5. Detect Collective Anomalies: To catch subtle trends, we examine if anomalies or high anomaly-scores occur in consecutive periods. A simple heuristic: if three or more months in a row show moderately high usage (even if each month alone is not extreme), we flag that sequence as a collective anomaly. More sophisticated approaches for collective anomalies include using time-series forecasting models (like Prophet or LSTM) to detect sustained deviations, but we will      illustrate a simple method for clarity.
6. Output Results: The code will output the identified anomaly points (with timestamps) and any detected anomaly trends.
Example Illustration: Below is a synthetic example of a school’s energy usage over time, with a single-month spike (point anomaly) and a 3-month sustained higher usage (collective anomaly). The anomalies are marked in red for visualization:
Example energy usage time series for a school, with a point anomaly (single spike) and a collective anomaly (3-month upward trend) highlighted in red.

How it works: We use IsolationForest on the monthly data with a contamination of 5% (meaning we expect ~5% of points to be anomalies). This model isolates outlier points based on the distribution of the data. Points identified with anomaly_label = -1 are printed as point anomalies with their date and usage value. Next, to find collective anomalies, we compute a rolling 3-month average of normalized usage (kWh per sqft). If any 3-month period’s average consumption is significantly higher than normal (here we used one standard deviation above the mean as a threshold) and those points were not all individually flagged already, we mark that period as an anomalous trend. The code prints these periods as date ranges.
Note: The detection of collective anomalies can be refined using more advanced models or domain knowledge. For example, one could train a time-series forecasting model (like Facebook Prophet or an LSTM autoencoder) on the normal behavior and then detect deviations that persist over multiple months. In practice, you would also account for known seasonality (e.g., use year-over-year comparisons for each month) and possibly weather normalization. The above approach is a heuristic to illustrate the concept.



## Part 2: Identifying Underperforming Buildings Post-Retrofit
In this part, we have two datasets (pre-retrofit and post-retrofit utility bills) for multiple schools (say 5 schools). Each dataset contains monthly energy usage per school. The goal is to find if any building is underperforming after the retrofit – meaning it did not achieve the expected energy savings (or has unusual usage patterns post-retrofit). This involves both trend anomalies (a school’s overall post-retrofit energy trend is off) and point anomalies (specific months post-retrofit that are abnormal for that school).
Approach:
1. Load Pre- and Post-Retrofit Data: Read the two CSV files preretrofit.csv and postretrofit.csv. Each should contain records of monthly energy usage for each school (with columns like School ID, billing date, electricity (kWh), gas, and square footage).
2. Normalize by Size: Compute energy use intensity (EUI) for each record, i.e., energy per square foot. This allows fair comparison between schools of different sizes. We can compute EUI for electricity and/or a combined energy metric (convert gas to kWh equivalent if needed, or handle separately).
3. Calculate Baseline vs Post-Retrofit Performance: For each school, aggregate the total (or average) annual EUI before and after retrofit. For example, sum the 12 months of kWh usage pre-retrofit and post-retrofit and divide by square footage to get annual EUI for each period. Compute the percentage change in EUI after retrofit.
4. Detect Underperformers (Building-Level Anomaly): We expect retrofits to reduce energy usage, so most schools should show a negative percentage change (i.e., lower EUI post-retrofit). An underperforming building might show much smaller reduction or even an increase in usage. We use an anomaly detection model (e.g., Isolation Forest or One-Class SVM) on the set of percentage changes to flag outliers. If one school’s performance is significantly worse than the others, it will be identified as an anomaly in this distribution. (With only ~5 schools, a simple statistical rule can also be used: e.g., any school with post-retrofit usage reduction less than, say, half the average reduction, or a positive increase, can be flagged.)
5. Identify Specific Anomalies in Usage Trends: For any flagged underperforming school, we can dig deeper. We would apply a similar approach as Part 1 to that school’s time series to find specific months or trends post-retrofit where usage was abnormally high (perhaps indicating faulty equipment or ineffective retrofit in one part of the campus).
6. Output Results: The code will list the school ID(s) that are detected as underperforming (if any), and could also highlight the anomaly points/trends in those schools.


Explanation: We first aggregate each school’s data to compute its pre- vs post-retrofit performance. Energy use per square foot (EUI) is used as the performance metric, since it normalizes for school size. We calculate the percentage change in annual EUI after the retrofit. Then we apply IsolationForest on the array of percentage changes to find any outlier. In our example, if one school’s usage went up or barely changed while others went down, it would likely be flagged as an anomaly. The code prints out any such underperforming school IDs and their EUI change.
Finally, for each flagged school, we optionally perform a more granular check on its post-retrofit monthly data (similar to Part 1) to identify specific months where usage was higher than expected. This uses a simple threshold (e.g., >10% above the pre-retrofit baseline EUI in a month) to highlight problem periods. In practice, one could again use a time-series model or control-chart approach on the individual school’s data for a detailed diagnostic.
By using these techniques, facility managers can automatically detect when a school’s energy usage is abnormal – either as a sudden spike or as a subtle drift upward – and take action. Moreover, by comparing multiple buildings, we can pinpoint if a particular school did not achieve the savings from a retrofit (perhaps due to an issue in one of its buildings or systems). This data-driven anomaly detection helps focus investigations on the right building and time period, enabling quicker resolution of energy waste issues.
